{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook brandnew biblical texts are generated. Select randomly a sequence of 100 characters from the Hebrew Bible \n",
    "and the model trained here will add 400 characters to it in the style of the Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geitb\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 5.5.14\n",
      "Api reference : https://dans-labs.github.io/text-fabric/Api/General/\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "114 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.09s B g_cons               from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.08s B language             from C:/Users/geitb/github/bhsa/tf/c\n",
      "  3.66s All features loaded/computed - for details use loadLog()\n",
      "   |     0.05s B otype                from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.45s B oslots               from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book                 from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B chapter              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B verse                from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.09s B g_cons               from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.13s B g_cons_utf8          from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.10s B g_lex                from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.14s B g_lex_utf8           from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.12s B g_word               from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.15s B g_word_utf8          from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.09s B lex0                 from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.17s B lex_utf8             from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B qere                 from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B qere_trailer         from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B qere_trailer_utf8    from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B qere_utf8            from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.07s B trailer              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.08s B trailer_utf8         from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B __levels__           from otype, oslots, otext\n",
      "   |     0.05s B __order__            from otype, oslots, __levels__\n",
      "   |     0.04s B __rank__             from otype, __order__\n",
      "   |     0.89s B __levUp__            from otype, oslots, __rank__\n",
      "   |     0.67s B __levDown__          from otype, __levUp__, __rank__\n",
      "   |     0.25s B __boundary__         from otype, oslots, __rank__\n",
      "   |     0.01s B __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     0.08s B language             from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@am              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@ar              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@bn              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@da              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@de              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@el              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@en              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@es              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@fa              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@fr              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@he              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@hi              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@id              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@ja              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@ko              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@la              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@nl              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@pa              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@pt              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@ru              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@sw              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@syc             from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@tr              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@ur              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@yo              from C:/Users/geitb/github/bhsa/tf/c\n",
      "   |     0.00s B book@zh              from C:/Users/geitb/github/bhsa/tf/c\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tf.fabric import Fabric\n",
    "DATABASE = '~/github'\n",
    "BHSA = 'bhsa/tf/c'\n",
    "TF = Fabric(locations=[DATABASE], modules=[BHSA], silent=False )\n",
    "\n",
    "api = TF.load('''\n",
    "      language g_cons\n",
    "''')\n",
    "\n",
    "api.loadLog()\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First generate the text of the Hebrew Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [F.g_cons.v(word) for word in F.otype.s(\"word\") if F.language.v(word) == \"Hebrew\"]\n",
    "    \n",
    "text = \" \".join(words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data are preprocessed. The text is cut in small pieces with maxlen characters and the data are converted to one-hot encoding.\n",
    "\n",
    "The input of the model consists of these pieces, the output is the character following the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 319703\n",
      "Number of unique characters: 25\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 100\n",
    "\n",
    "# We create a new sequence every \"step\" characters\n",
    "step = 5\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Extracting sentences and the next characters.\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print('Number of sentences:', len(sentences))\n",
    "\n",
    "# Extracting unique characters from the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Number of unique characters:', len(chars))\n",
    "\n",
    "# Dictionary for mapping unique characters to their index\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Converting characters into one-hot encoding.\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the model is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))    \n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model is trained and new texts are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "319703/319703 [==============================] - 429s 1ms/step - loss: 1.9420\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.94198, saving model to weights-01-1.9420.hdf5\n",
      "--- Seeded text: \" W DBC W CMN W YRJ NTNW M<RBK DMFQ SXRTK B RB M<FJK M RB KL HWN B JJN XLBWN W YMR YXR WDN W JWN M >W\"\n",
      " W DBC W CMN W YRJ NTNW M<RBK DMFQ SXRTK B RB M<FJK M RB KL HWN B JJN XLBWN W YMR YXR WDN W JWN M >WL KL RGP CLX LJ H >CR YR< Z>XTJ MLK BJT CMJM MMNWN >XW W TDBRTM W JMT KPR CBR <MWN <NJH W JD< <L KL >JC LVW PLCTW JDWN B SL W <L CB< L HJWT >T W RYH W J>MR DBRW HNNJ NH JFMXW >TN B NPC >LP XKM KL TJSW >T KL H QWMJM CNK >T H M<DJ LHL KJ NXRT HTNPLR W J>MR <LJHWN >TB<V B JDW W L> TWKWS MLL H <MJM  LBJM >GRN Q>WRJ >XRJ DWMR>D BJN W MPQH W CMH YRJM W <L P>T >HLK W >BD W JGDRW KL H NMLLJM W >MR >T H GBepoch 2\n",
      "Epoch 1/1\n",
      "319703/319703 [==============================] - 436s 1ms/step - loss: 1.7504\n",
      "\n",
      "Epoch 00001: loss improved from 1.94198 to 1.75041, saving model to weights-01-1.7504.hdf5\n",
      "--- Seeded text: \"Q XWNN W NWTN KJ MBRKJW JJRCW >RY W MQLLJW JKRTW M JHWH MY<DJ GBR KWNNW W DRKW JXPY KJ JPL L> JWVL K\"\n",
      "Q XWNN W NWTN KJ MBRKJW JJRCW >RY W MQLLJW JKRTW M JHWH MY<DJ GBR KWNNW W DRKW JXPY KJ JPL L> JWVL KJ L> >MK >LTH ZRJB K JFR>L W JRNK PBR KJ >JC QJNK >ZR >XJ >WTJ B MZRKX MLM >RYJM JFR>L W HPCTJM BN XFJFRH KJ HW> NGD LNP NXWPH B XNW <WGD XX PN RBWT LB >L FRJ DBNJM RQBJM MCB<T YLJM HTLK >JC >JC W JTLM W GM H R< W HJH W JMN <L TMKXTH L LB H XQRJM QWMJMNH W JCBW MN NGD MWM W B<RH JMJM RKCT L XWJ CW W ><LH HQLLJ KJ >L TMWTJ R<H N>FJW HW JBJNJW KW> BJT <DRZH KJ >DNJ CDJM WNJ YDQ W >JC >CR T<FJM W W Jepoch 3\n",
      "Epoch 1/1\n",
      "319703/319703 [==============================] - 412s 1ms/step - loss: 1.7061\n",
      "\n",
      "Epoch 00001: loss improved from 1.75041 to 1.70607, saving model to weights-01-1.7061.hdf5\n",
      "--- Seeded text: \"JM <D >CR L> JRXQ XBL H KSP W TRY GLT H ZHB W TCBR KD <L H MBW< W NRY H GLGL >L H BWR W JCB H <PR <L\"\n",
      "JM <D >CR L> JRXQ XBL H KSP W TRY GLT H ZHB W TCBR KD <L H MBW< W NRY H GLGL >L H BWR W JCB H <PR <L DWJD NXBBH W JLK BQCW PN JDJ W J>MRW KL H M<BRH W HNH H GNBWTJK ><L JFR>L W HCBX >L H >RY W L> CMWV TXRHW KLMH W J>MR L> NTN H MWY>DW B MWT CBR L QDCJ CPK <L JDJ W KL H BJT W M R< JHWH L HBJ<W MMNH B >CR FQWYR <LJHW <MK TMW W B MKBNW XSRW <L JHWH YDQ BTK TXTJ >L M<BJT FH NPC JHWH >PR >JC B KL KL HW> JHWH MMLKBW K >MR >NKJ K >CR LK CM B QRBKM TBW>J LK KJ < W PRC< BRJW HXLJB BJT JY>JK B <FW <BDK JHepoch 4\n",
      "Epoch 1/1\n",
      "319703/319703 [==============================] - 425s 1ms/step - loss: 1.7972\n",
      "\n",
      "Epoch 00001: loss did not improve from 1.70607\n",
      "--- Seeded text: \"JT BJT JHWDH <LJHM JR<WN B BTJ >CQLWN B  <RB JRBYWN KJ JPQDM JHWH >LHJHM W CB CBWTM CM<TJ XRPT MW>B \"\n",
      "JT BJT JHWDH <LJHM JR<WN B BTJ >CQLWN B  <RB JRBYWN KJ JPQDM JHWH >LHJHM W CB CBWTM CM<TJ XRPT MW>B JQRJB <"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geitb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LJJLH W PWSPN J CHJHM KH M  HSBJM HJ PWTWH <LJ >LH B  PW >MJMJM M  JM MJ HWLJM H >KW W TDJHM >JC JD W BKM MXJ H CJLWW >RYJW MN H JJN GCQ W JMH W H JWJN >CJH H MYRH L> JBNW JJMWR W NHJ MXW B> TPT >CR <W HWLJD BJTNJ JMJM HM <M <D P<RJH >RB<JM W H >RY HJX H JTR M DLL <L MZBXJ LJ KJ L> HKH M PJM LBBK M >CH J<NJW W <TH B TXJW W JXNH W >M HHTMLNW NQMH NJLH <MJM L YPHWR L WHM YPHJ MJ JT JMJNK  NJW"
     ]
    }
   ],
   "source": [
    "# function sample converts predictions to probabilities and chooses the most probable with a certain randomness to\n",
    "# avoid repetition in the prediction\n",
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch\n",
    "    model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=callbacks_list)\n",
    "    \n",
    " \n",
    "    # Select a text seed randomly\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Seeded text: \"' + generated_text + '\"')\n",
    "\n",
    "    sys.stdout.write(generated_text)\n",
    "\n",
    "    # We generate 400 characters        \n",
    "    for i in range(400):\n",
    "            \n",
    "        # first select a text randomly and convert it to one hot\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.\n",
    "                \n",
    "        # predict next character based on sampled text\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = chars[next_index]\n",
    "            \n",
    "        # add new character to generated text, remove the first character and make new prediction\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
